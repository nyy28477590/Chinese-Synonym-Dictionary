{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1268: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 articles processed.\n",
      "20000 articles processed.\n",
      "30000 articles processed.\n",
      "40000 articles processed.\n",
      "50000 articles processed.\n",
      "60000 articles processed.\n",
      "70000 articles processed.\n",
      "80000 articles processed.\n",
      "90000 articles processed.\n",
      "100000 articles processed.\n",
      "110000 articles processed.\n",
      "120000 articles processed.\n",
      "130000 articles processed.\n",
      "140000 articles processed.\n",
      "150000 articles processed.\n",
      "160000 articles processed.\n",
      "170000 articles processed.\n",
      "180000 articles processed.\n",
      "190000 articles processed.\n",
      "200000 articles processed.\n",
      "210000 articles processed.\n",
      "220000 articles processed.\n",
      "230000 articles processed.\n",
      "240000 articles processed.\n",
      "250000 articles processed.\n",
      "260000 articles processed.\n",
      "270000 articles processed.\n",
      "280000 articles processed.\n",
      "290000 articles processed.\n",
      "300000 articles processed.\n",
      "310000 articles processed.\n",
      "320000 articles processed.\n",
      "330000 articles processed.\n",
      "340000 articles processed.\n",
      "350000 articles processed.\n",
      "360000 articles processed.\n",
      "370000 articles processed.\n",
      "380000 articles processed.\n",
      "390000 articles processed.\n",
      "400000 articles processed.\n",
      "407222 articles processed.\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "wiki_corpus = WikiCorpus('zhwiki-latest-pages-articles-multistream.xml.bz2', dictionary={})\n",
    "text_num = 0\n",
    "\n",
    "with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in wiki_corpus.get_texts():\n",
    "        f.write(' '.join(text)+'\\n')\n",
    "        text_num += 1\n",
    "        if text_num % 10000 == 0:\n",
    "            print('{} articles processed.'.format(text_num))\n",
    "\n",
    "    print('{} articles processed.'.format(text_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\nyy28\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.164 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "407222it [1:58:38, 57.21it/s] \n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import jieba\n",
    "from opencc import OpenCC\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initial\n",
    "cc = OpenCC('s2t')\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "with open('wiki_text_seg.txt', 'w', encoding='utf-8') as new_f:\n",
    "    with open('wiki_text.txt', 'r', encoding='utf-8') as f:\n",
    "        for times, data in tqdm(enumerate(f, 1)):\n",
    "            data = cc.convert(data)\n",
    "            data = jieba.cut(data)\n",
    "            data = [word for word in data if word != ' ']\n",
    "            data = ' '.join(data)\n",
    "\n",
    "            new_f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Settings\n",
    "seed = 666\n",
    "sg = 0\n",
    "window_size = 10\n",
    "vector_size = 100\n",
    "min_count = 1\n",
    "workers = 8\n",
    "epochs = 5\n",
    "batch_words = 10000\n",
    "\n",
    "train_data = word2vec.LineSentence('wiki_text_seg.txt')\n",
    "model = Word2Vec(\n",
    "    train_data,\n",
    "    min_count=min_count,\n",
    "    size=vector_size,\n",
    "    workers=workers,\n",
    "    iter=epochs,\n",
    "    window=window_size,\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    batch_words=batch_words\n",
    ")\n",
    "\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('新竹市', 0.7741459608078003)\n",
      "('高雄市', 0.7578689455986023)\n",
      "('五股', 0.7470508813858032)\n",
      "('桃園市', 0.7453809976577759)\n",
      "('新北', 0.7444837093353271)\n",
      "('中和區', 0.7410485744476318)\n",
      "('桃園', 0.7308928966522217)\n",
      "('新莊區', 0.7241247296333313)\n",
      "('中壢區', 0.7220543026924133)\n",
      "('基隆市', 0.7216924428939819)\n",
      "('汐止', 0.7191495895385742)\n",
      "('蘆洲區', 0.7181679010391235)\n",
      "('嘉義市', 0.7168759703636169)\n",
      "('八德', 0.7164942026138306)\n",
      "('林口', 0.7155312299728394)\n",
      "('樹林區', 0.7107371091842651)\n",
      "('蘆洲', 0.7097631692886353)\n",
      "('三峽區', 0.7097036838531494)\n",
      "('新店', 0.702294647693634)\n",
      "('板橋區', 0.6884027123451233)\n",
      "('北市', 0.6827808022499084)\n",
      "('三芝', 0.6776130795478821)\n",
      "('歌區', 0.676001250743866)\n",
      "('蘆竹區', 0.6715206503868103)\n",
      "('萬華', 0.6700690388679504)\n",
      "('今新北市', 0.6686965823173523)\n",
      "('三重市', 0.666114866733551)\n",
      "('為桃園', 0.6612262725830078)\n",
      "('楊梅區', 0.6598970890045166)\n",
      "('桃園區', 0.6547895073890686)\n",
      "('北投', 0.652437150478363)\n",
      "('中和市', 0.6492843627929688)\n",
      "('宜蘭縣', 0.6461074352264404)\n",
      "('信義區', 0.6453157663345337)\n",
      "('嘉義縣', 0.6450932621955872)\n",
      "('新莊', 0.6431164145469666)\n",
      "('雲林縣', 0.642916202545166)\n",
      "('內湖', 0.6419414281845093)\n",
      "('莊岫樵', 0.6413303017616272)\n",
      "('深坑', 0.6409517526626587)\n",
      "('大溪', 0.6395325064659119)\n",
      "('年桃園', 0.6374632120132446)\n",
      "('平鎮區', 0.6364297270774841)\n",
      "('板橋市', 0.6353011727333069)\n",
      "('晏熙', 0.6292755007743835)\n",
      "('士林', 0.6285300254821777)\n",
      "('安坑', 0.6281789541244507)\n",
      "('高雄', 0.6272528171539307)\n",
      "('景美', 0.6262185573577881)\n",
      "('永和市', 0.624852180480957)\n"
     ]
    }
   ],
   "source": [
    "for item in model.wv.most_similar('新北市', topn=50):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44202584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('開發', '開拓')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.09629527e-01,  1.86820671e-01, -9.38649923e-02, -3.71760502e-02,\n",
       "       -1.49240419e-01, -8.35591033e-02,  8.03623945e-02,  2.35189404e-02,\n",
       "       -1.68884456e-01, -6.36756793e-02, -1.45291924e-01, -2.34010279e-01,\n",
       "        7.00463727e-02, -1.01008110e-01, -5.46279512e-02, -8.31140056e-02,\n",
       "        7.85085857e-02, -6.29560798e-02,  1.23394385e-01, -9.40548489e-05,\n",
       "        1.03192963e-01,  2.57415295e-01, -1.50014251e-01, -6.46368936e-02,\n",
       "       -7.92484507e-02,  1.37421899e-02, -1.12662382e-01, -1.11084677e-01,\n",
       "        2.58784555e-02,  1.60199359e-01, -4.39065211e-02,  1.21686533e-01,\n",
       "        1.00409761e-01, -2.29802597e-02,  1.46962414e-02,  5.61321974e-02,\n",
       "       -6.34788498e-02, -2.53924858e-02, -1.03679784e-02, -6.57196939e-02,\n",
       "       -1.83722153e-01,  4.42073010e-02, -4.54467908e-02, -1.23534827e-02,\n",
       "        1.27629355e-01,  7.08319247e-02, -8.80627930e-02, -1.60612899e-03,\n",
       "       -4.65670153e-02, -1.27489462e-01,  4.75783646e-02,  1.06541112e-01,\n",
       "        6.22765049e-02,  1.39225334e-01, -8.41537490e-02, -1.33978734e-02,\n",
       "       -1.97273716e-01, -1.39301926e-01, -7.90145621e-02,  1.74171448e-01,\n",
       "       -9.98169854e-02, -1.19923063e-01,  8.84445310e-02,  1.07990146e-01,\n",
       "        4.49677557e-02,  1.00227594e-01,  2.00589150e-02, -4.65176143e-02,\n",
       "       -2.38858685e-02, -4.87377234e-02, -1.63757876e-01, -4.17122245e-02,\n",
       "       -2.70635206e-02,  1.98824494e-03,  3.12413592e-02, -1.48817927e-01,\n",
       "       -4.53813113e-02,  3.53538133e-02, -5.20810634e-02,  1.63773537e-01,\n",
       "        2.11596508e-02, -6.21712692e-02, -3.25704068e-02,  2.38119408e-01,\n",
       "        1.08545432e-02, -8.48060250e-02,  8.78034234e-02,  1.35698527e-01,\n",
       "        8.06827024e-02, -2.69243121e-02,  1.43839836e-01, -2.65477076e-02,\n",
       "       -5.11434581e-03, -3.41298617e-02, -4.46162112e-02,  5.63128330e-02,\n",
       "        5.79912215e-02,  6.05165865e-03,  4.56415303e-02,  1.23575084e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['台灣']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Similarity (Calculate Cosine Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import jieba.analyse\n",
    "import jieba\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "\n",
    "#model = word2vec.Word2Vec.load('word2vec.model')\n",
    "\n",
    "def vector_similarity(s1, s2):\n",
    "    def sentence_vector(s):\n",
    "        words = jieba.analyse.extract_tags(s, topK=5)\n",
    "        if len(words)<1:\n",
    "            print('無關鍵字')\n",
    "            return 0\n",
    "        #words = jieba.lcut(s)\n",
    "        v = np.zeros(100)\n",
    "        for word in words:\n",
    "            try:\n",
    "                v += model.wv[word]\n",
    "            except:\n",
    "                continue\n",
    "        v /= len(words)\n",
    "        return v\n",
    "    \n",
    "    v1, v2 = sentence_vector(s1), sentence_vector(s2)\n",
    "    return np.dot(v1, v2) / (norm(v1) * norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = '早安'\n",
    "s2 = '早安'\n",
    "vector_similarity(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
